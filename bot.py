import os
import asyncio
import logging
import re
import random
import sys
import json
import base64
from datetime import datetime, timedelta, timezone
from urllib.parse import urlparse, urljoin
from typing import Dict, Any, List, Optional

import asyncpg
import aiohttp
import feedparser
from bs4 import BeautifulSoup

from aiogram import Bot, Dispatcher, types, F
from aiogram.enums import ParseMode
from aiogram.filters import Command
from aiogram.client.default import DefaultBotProperties
from aiogram.types import InlineKeyboardMarkup, InlineKeyboardButton
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import State, StatesGroup

KYIV_TZ = timezone(timedelta(hours=3), 'Europe/Kyiv')

logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                    stream=sys.stdout)
logger = logging.getLogger(__name__)

BOT_TOKEN = os.getenv("BOT_TOKEN")
DATABASE_URL = os.getenv("DATABASE_URL")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")

try:
    channel_env_var = os.getenv("CHANNEL_ID") or os.getenv("channel_ID") 
    CHANNEL_ID = int(channel_env_var) if channel_env_var else None
    ADMIN_ID = int(os.getenv("ADMIN_ID", CHANNEL_ID or 0)) 
except (TypeError, ValueError):
    CHANNEL_ID = None
    ADMIN_ID = 0
    logger.error("CHANNEL_ID –∞–±–æ ADMIN_ID –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∞–±–æ –º–∞—î –Ω–µ–∫–æ—Ä–µ–∫—Ç–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç.")

GEMINI_API_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent"
random.seed(42)

class EconomicConfig:
    
    OLX_SEARCH_QUERIES = [ 
        '—á–æ–ª–æ–≤—ñ—á–∏–π –≥–æ–¥–∏–Ω–Ω–∏–∫ rolex', '–≤–µ—Ä—Å—Ç–∞—Ç —á–ø—É –ø—Ä–æ–º–∏—Å–ª–æ–≤–∏–π', '–º–æ–Ω–µ—Ç–∞ 5 —Ä—É–±–ª—ñ–≤ –∑–æ–ª–æ—Ç–æ', 
        '—Ü–∞—Ä—Å—å–∫–µ —Å—Ä—ñ–±–ª–æ', '—ñ–∫–æ–Ω–∞ —Å—Ä—ñ–±–ª–æ', '–∑–≤–∞—Ä—é–≤–∞–ª—å–Ω–∏–π –∞–ø–∞—Ä–∞—Ç kemppi' 
    ]
    OLX_SCRAP_QUERIES = [ 
        '–∑–æ–ª–æ—Ç–æ –ª–æ–º 585', '—Å—Ä—ñ–±–ª–æ –ª–æ–º 925', '–∑–æ–ª–æ—Ç–∏–π –∑–ª–∏—Ç–æ–∫' 
    ]
    OLX_PRICE_FILTER = 20000 
    
    TRANSACTION_FEES_PERCENT = 10  
    MIN_PROFIT_MARGIN_PERCENT = 20 
    
    MACHINERY_DEPRECIATION_RATE = 0.05  
    MACHINERY_CONDITION_WEIGHT = 0.4    
    MACHINERY_HOURS_PENALTY_RATE = 0.000005 
    MIN_RARITY_SCORE = 50 
    MAX_AUTHENTICITY_RISK = 30
    PRESTIGE_MULTIPLIERS = {'rolex': 1.5, 'patek philippe': 1.8, 'omega': 1.3}
    FEEDBACK_CORRECTION_MULTIPLIER = 0.1 

    SPOT_PRICES: Dict[str, float] = {
        "GOLD_585_UAH_PER_GRAM": 2850.0, 
        "SILVER_925_UAH_PER_GRAM": 48.5, 
        "LAST_UPDATED": datetime.now(KYIV_TZ).replace(hour=0, minute=0, second=0, microsecond=0).timestamp()
    }
    
    SILVER_KEYWORDS: List[str] = [
        "—Å—Ä—ñ–±–ª–æ", "—Å—Ä—ñ–±–Ω–∏–π", "Ag 925", "sterling silver", "800 –ø—Ä–æ–±–∞", "925 –ø—Ä–æ–±–∞", "–ø–æ—Å—Ä—ñ–±–ª–µ–Ω–∏–π", "–º–µ–ª—å—Ö—ñ–æ—Ä"
    ]
    
    RSS_FEEDS: Dict[str, str] = {
        "UKR Finance News": "https://www.rbc.ua/static/rss/news.ukr.rss", 
        "Metals Market News": "https://www.google.com/search?q=—Ü—ñ–Ω–∏+–Ω–∞+—Å—Ä—ñ–±–ª–æ+–Ω–æ–≤–∏–Ω–∏&tbm=nws&output=rss"
    }

class BaseForm(StatesGroup):
    waiting_for_photo = State()
    waiting_for_text = State()

class CutleryAnalysis(StatesGroup):
    waiting_for_url_or_description = State()

class LearningState(StatesGroup):
    waiting_for_topic = State()
    in_session = State()
    waiting_for_next = State()

async def init_db(pool: asyncpg.Pool):
    logger.info("–ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è —Ç–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ë–î...")
    await pool.execute("""
        CREATE TABLE IF NOT EXISTS olx_posts (
            id SERIAL PRIMARY KEY,
            olx_id TEXT UNIQUE,
            title TEXT,
            price INTEGER,
            published_at TIMESTAMP WITH TIME ZONE,
            ai_analysis_json JSONB,
            is_relevant BOOLEAN
        );
    """)
    await pool.execute("""
        CREATE TABLE IF NOT EXISTS user_base (
            id SERIAL PRIMARY KEY,
            user_id BIGINT,
            title TEXT,
            image_url TEXT,
            keywords TEXT,
            estimated_value_text TEXT,
            created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
        );
    """)
    await pool.execute("""
        CREATE TABLE IF NOT EXISTS user_feedback (
            id SERIAL PRIMARY KEY,
            user_id BIGINT,
            olx_id TEXT,
            is_like BOOLEAN,
            created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
        );
    """)
    await pool.execute("""
        CREATE TABLE IF NOT EXISTS users (
            user_id BIGINT PRIMARY KEY,
            username TEXT,
            joined_at TIMESTAMP WITH TIME ZONE
        );
    """)
    logger.info("–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ë–î –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")


async def fetch_page_content(session: aiohttp.ClientSession, url: str) -> str | None:
    try:
        parsed_url = urlparse(url)
        if not all([parsed_url.scheme in ['http', 'https'], parsed_url.netloc]): 
             logger.warning(f"–ù–µ–∫–æ—Ä–µ–∫—Ç–Ω–∏–π URL: {url}")
             return None

        headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}
        async with session.get(url, headers=headers, timeout=10) as response:
            if response.status != 200: 
                logger.warning(f"–ü–æ–º–∏–ª–∫–∞ HTTP {response.status} –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ {url}")
                return None
            return await response.text()
    except aiohttp.ClientError as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ aiohttp –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ {url}: {e}")
        return None
    except Exception as e:
        logger.error(f"–ù–µ–≤—ñ–¥–æ–º–∞ –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ {url}: {e}")
        return None

async def get_image_base64(session, url, bot: Optional[Bot]=None, file_id=None):
    if file_id and bot:
        try:
            tg_file = await bot.get_file(file_id)
            url = f"https://api.telegram.org/file/bot{BOT_TOKEN}/{tg_file.file_path}"
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è file_path Telegram: {e}")
            return None
    
    if not url: return None

    try:
        async with session.get(url, timeout=15) as response:
            response.raise_for_status()
            image_data = await response.read()
            if len(image_data) > 4 * 1024 * 1024:
                logger.warning("–ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑–∞–Ω–∞–¥—Ç–æ –≤–µ–ª–∏–∫–µ (>4MB). –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ Vision Analysis.")
                return None
            return base64.b64encode(image_data).decode('utf-8')
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑ {url}: {e}")
        return None

async def gemini_api_call(session: aiohttp.ClientSession, payload: Dict[str, Any]):
    if not GEMINI_API_KEY:
        logger.warning("Gemini API Key –≤—ñ–¥—Å—É—Ç–Ω—ñ–π. –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ API-–≤–∏–∫–ª–∏–∫.")
        return None
    
    max_retries = 3
    delay = 1 

    for attempt in range(max_retries):
        try:
            async with session.post(f"{GEMINI_API_URL}?key={GEMINI_API_KEY}", json=payload, timeout=30) as response:
                if response.status == 200:
                    result = await response.json()
                    json_str = result['candidates'][0]['content']['parts'][0]['text']
                    return json.loads(json_str)
                
                logger.warning(f"Gemini API –ø–æ–º–∏–ª–∫–∞ {response.status}. –°–ø—Ä–æ–±–∞ {attempt+1}/{max_retries}.")
                await asyncio.sleep(delay)
                delay *= 2 
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –≤–∏–∫–ª–∏–∫—É Gemini API: {e}. –°–ø—Ä–æ–±–∞ {attempt+1}/{max_retries}.")
            await asyncio.sleep(delay)
            delay *= 2
            
    return None

async def _get_adaptive_system_instruction(pool: asyncpg.Pool, user_id: int, is_vision: bool = True) -> str:
    async with pool.acquire() as conn:
        user_base_records = await conn.fetch("""
            SELECT title, keywords, estimated_value_text FROM user_base 
            WHERE user_id = $1 ORDER BY created_at DESC LIMIT 5
        """, user_id)
        
        user_context = "–ù–∞—Å—Ç—É–ø–Ω—ñ –µ—Ç–∞–ª–æ–Ω–Ω—ñ –ø—Ä–µ–¥–º–µ—Ç–∏ –¥–æ–¥–∞–Ω–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–º:\n"
        if user_base_records:
            for rec in user_base_records:
                user_context += f"- **{rec['title']}** (–ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞: {rec['keywords']}. –û—Ü—ñ–Ω–∫–∞: {rec['estimated_value_text']})\n"
        else:
            user_context += "- –î–∞–Ω—ñ –≤—ñ–¥—Å—É—Ç–Ω—ñ.\n"
            
        if not is_vision: 
             disputed_posts = await conn.fetch("""
                SELECT 
                    olx_posts.title,
                    SUM(CASE WHEN user_feedback.is_like = TRUE THEN 1 ELSE 0 END) AS likes,
                    SUM(CASE WHEN user_feedback.is_like = FALSE THEN 1 ELSE 0 END) AS dislikes
                FROM olx_posts
                JOIN user_feedback ON olx_posts.olx_id = user_feedback.olx_id
                GROUP BY olx_posts.title
                HAVING SUM(CASE WHEN user_feedback.is_like = TRUE THEN 1 ELSE 0 END) > 0 AND 
                       SUM(CASE WHEN user_feedback.is_like = FALSE THEN 1 ELSE 0 END) > 0
                ORDER BY ABS(likes - dislikes) ASC, (likes + dislikes) DESC LIMIT 3;
            """)
             
             disputed_context = "\n–ù–∞–π–±—ñ–ª—å—à —Å–ø—ñ—Ä–Ω—ñ –ø—Ä–µ–¥–º–µ—Ç–∏, —è–∫—ñ –ø–æ—Ç—Ä–µ–±—É—é—Ç—å –ø–æ–∫—Ä–∞—â–µ–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É:\n"
             if disputed_posts:
                 for post in disputed_posts:
                     disputed_context += f"- **{post['title']}** (–õ–∞–π–∫–∏: {post['likes']}, –î–∏–∑–ª–∞–π–∫–∏: {post['dislikes']})\n"
             
        
    base_instruction = "–í–∏ ‚Äî –¥–æ—Å–≤—ñ–¥—á–µ–Ω–∏–π AI-–µ–∫—Å–ø–µ—Ä—Ç, —â–æ –≤–æ–ª–æ–¥—ñ—î –º–æ–¥–µ–ª—è–º–∏ TIV/RAV –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É —ñ–Ω–≤–µ—Å—Ç–∏—Ü—ñ–π–Ω–∏—Ö –∞–∫—Ç–∏–≤—ñ–≤."
    
    if is_vision:
        return f"{base_instruction} –í–∞—à–∞ –º–µ—Ç–∞ ‚Äî —Ç–æ—á–Ω–æ –∫–ª–∞—Å–∏—Ñ—ñ–∫—É–≤–∞—Ç–∏ –∞–∫—Ç–∏–≤ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ç–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞. –ó–≤–µ—Ä–Ω—ñ—Ç—å —É–≤–∞–≥—É –Ω–∞:\n{user_context}"
    else:
        return f"{base_instruction} –í–∞—à–∞ –º–µ—Ç–∞ ‚Äî –Ω–∞–≤—á–∞—Ç–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ —Å–∫–ª–∞–¥–Ω–∏–º –∞—Å–ø–µ–∫—Ç–∞–º —ñ–Ω–≤–µ—Å—Ç–∏—Ü—ñ–π–Ω–æ–≥–æ –∫–æ–ª–µ–∫—Ü—ñ–æ–Ω—É–≤–∞–Ω–Ω—è. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ –∑–Ω–∞–Ω–Ω—è –ø—Ä–æ —Å–ø—ñ—Ä–Ω—ñ –∞–∫—Ç–∏–≤–∏ –¥–ª—è –Ω–∞–≥–æ–ª–æ—à–µ–Ω–Ω—è –Ω–∞ –∫–ª—é—á–æ–≤–∏—Ö —Ä–∏–∑–∏–∫–∞—Ö: \n{disputed_context}"

async def gemini_vision_analysis(session, prompt, image_base64, pool: asyncpg.Pool, user_id: int, response_schema):
    if not image_base64: return None
    
    system_instruction = await _get_adaptive_system_instruction(pool, user_id, is_vision=True)
    
    payload = {
        "contents": [
            {
                "role": "user",
                "parts": [
                    {"text": prompt},
                    {
                        "inlineData": {
                            "mimeType": "image/jpeg", 
                            "data": image_base64
                        }
                    }
                ]
            }
        ],
        "config": {
            "responseMimeType": "application/json",
            "responseSchema": response_schema,
        },
        "systemInstruction": {"parts": [{"text": system_instruction}]}
    }
    return await gemini_api_call(session, payload)

async def generate_collector_lesson(session: aiohttp.ClientSession, topic: str, pool: asyncpg.Pool, user_id: int, difficulty: str = "intermediate") -> Optional[Dict[str, Any]]:
    system_prompt = await _get_adaptive_system_instruction(pool, user_id, is_vision=False)
    
    lesson_schema = {
        "type": "OBJECT",
        "properties": {
            "lesson_title": {"type": "STRING", "description": "–ù–∞–∑–≤–∞ —É—Ä–æ–∫—É."},
            "content": {"type": "STRING", "description": "–î–µ—Ç–∞–ª—å–Ω–∏–π, –Ω–∞–≤—á–∞–ª—å–Ω–∏–π —Ç–µ–∫—Å—Ç —É—Ä–æ–∫—É, –∑ —Ä–æ–∑–±–∏–≤–∫–æ—é –Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∏ (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ **–∂–∏—Ä–Ω–∏–π** —Ç–µ–∫—Å—Ç –¥–ª—è —Ç–µ—Ä–º—ñ–Ω—ñ–≤)."},
            "quiz_question": {"type": "STRING", "description": "–û–¥–Ω–µ –ø–∏—Ç–∞–Ω–Ω—è –¥–ª—è –≤—ñ–∫—Ç–æ—Ä–∏–Ω–∏."},
            "quiz_answer": {"type": "STRING", "description": "–ö–æ—Ä–æ—Ç–∫–∞, –ø—Ä–∞–≤–∏–ª—å–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è."},
            "quiz_hint": {"type": "STRING", "description": "–ü—ñ–¥–∫–∞–∑–∫–∞ –¥–ª—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞, —è–∫—â–æ –≤—ñ–Ω –ø–æ–º–∏–ª–∏—Ç—å—Å—è."}
        },
        "required": ["lesson_title", "content", "quiz_question", "quiz_answer", "quiz_hint"]
    }
    
    prompt = f"–ó–≥–µ–Ω–µ—Ä—É–π —ñ–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∏–π —É—Ä–æ–∫ –Ω–∞ —Ç–µ–º—É '{topic}' –¥–ª—è —Ä—ñ–≤–Ω—è '{difficulty}'. –í–∫–ª—é—á–∏ –∫–ª—é—á–æ–≤—ñ —Ç–µ—Ä–º—ñ–Ω–∏, —Ä–∏–∑–∏–∫–∏ —Ç–∞ –ø–æ—Ä–∞–¥–∏ —â–æ–¥–æ –æ—Ü—ñ–Ω–∫–∏. –°—Ç–≤–æ—Ä–∏ –æ–¥–Ω–µ –ø–∏—Ç–∞–Ω–Ω—è –¥–ª—è –≤—ñ–∫—Ç–æ—Ä–∏–Ω–∏ –∑ –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é —Ç–∞ –ø—ñ–¥–∫–∞–∑–∫–æ—é."
    
    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {
            "responseMimeType": "application/json",
            "responseSchema": lesson_schema
        },
        "systemInstruction": {"parts": [{"text": system_prompt}]}
    }

    result = await gemini_api_call(session, payload)
    return result

def analyze_for_silver(content: str) -> List[str]:
    if '<html' in content.lower():
        soup = BeautifulSoup(content, 'html.parser')
        text = soup.get_text()
    else:
        text = content
        
    normalized_text = text.lower()
    found_keywords = []
    
    for keyword in EconomicConfig.SILVER_KEYWORDS:
        if re.search(r'\b' + re.escape(keyword.lower()) + r'\b', normalized_text) or keyword.lower() in normalized_text:
            if keyword not in found_keywords:
                found_keywords.append(keyword)
                
    return found_keywords

def generate_high_yield_proposal() -> str:
    asset_list = ["–¢–µ—Ö–Ω–æ–ª–æ–≥—ñ—á–Ω–∏–π ETF (–°–∏–º—É–ª—è—Ü—ñ—è)", "–†—ñ–¥–∫—ñ—Å–Ω—ñ –º–µ—Ç–∞–ª–∏ (–°–∏–º—É–ª—è—Ü—ñ—è)", "–ê–∫—Ü—ñ—ó '–ó–µ–ª–µ–Ω–æ—ó' –ï–Ω–µ—Ä–≥–µ—Ç–∏–∫–∏ (–°–∏–º—É–ª—è—Ü—ñ—è)", "–§'—é—á–µ—Ä—Å–∏ –Ω–∞ –µ–∫–∑–æ—Ç–∏—á–Ω–∏–π —Ç–æ–≤–∞—Ä (–°–∏–º—É–ª—è—Ü—ñ—è)"]
    asset = random.choice(asset_list)
    risk = random.choice(["–ï–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ –≤–∏—Å–æ–∫–∏–π", "–ù–∞–¥–∑–≤–∏—á–∞–π–Ω–æ –≤–∏—Å–æ–∫–∏–π", "–í–∏—Å–æ–∫–∏–π, –Ω–µ –¥–ª—è –Ω–æ–≤–∞—á–∫—ñ–≤"])
    duration = random.choice(["3 –º—ñ—Å—è—Ü—ñ", "6 –º—ñ—Å—è—Ü—ñ–≤", "12 –º—ñ—Å—è—Ü—ñ–≤"])
    simulated_return = random.randint(400, 650)
    
    return (
        f"üö® **–ï–õ–Ü–¢–ù–ê –í–ò–°–û–ö–û–†–ò–ó–ò–ö–û–í–ê –°–ò–ú–£–õ–Ø–¶–Ü–ô–ù–ê –ü–†–û–ü–û–ó–ò–¶–Ü–Ø** üö®\n\n"
        f"üìà **–ü–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∏–π –ü—Ä–∏–±—É—Ç–æ–∫ (–°–∏–º—É–ª—è—Ü—ñ—è):** `{simulated_return}%`\n"
        f"üéØ **–ê–∫—Ç–∏–≤:** *{asset}*\n"
        f"‚è≥ **–û–±—Ä—ñ–π:** {duration}\n" 
        f"‚ö†Ô∏è **–†—ñ–≤–µ–Ω—å –†–∏–∑–∏–∫—É:** *{risk}*\n\n"
        f"üìù **–ê–Ω–∞–ª—ñ–∑:** –¶–µ —Å–∏–º—É–ª—å–æ–≤–∞–Ω–∞ '–ø—Ä–æ–ø–æ–∑–∏—Ü—ñ—è', —â–æ –≤—ñ–¥–æ–±—Ä–∞–∂–∞—î –≥—ñ–ø–æ—Ç–µ—Ç–∏—á–Ω—É —Å–∏—Ç—É–∞—Ü—ñ—é "
        f"–Ω–∞ –µ–∫–∑–æ—Ç–∏—á–Ω–∏—Ö —Ä–∏–Ω–∫–∞—Ö. –¢–∞–∫–∏–π –ø—Ä–∏–±—É—Ç–æ–∫ –º–æ–∂–ª–∏–≤–∏–π –ª–∏—à–µ —É —Ä–∞–∑—ñ –ø—Ä–∏–π–Ω—è—Ç—Ç—è "
        f"–Ω–∞–¥–∑–≤–∏—á–∞–π–Ω–æ –≤–∏—Å–æ–∫–∏—Ö —Ä–∏–∑–∏–∫—ñ–≤, –≤–∫–ª—é—á–∞—é—á–∏ –ø–æ–≤–Ω—É –≤—Ç—Ä–∞—Ç—É –∫–∞–ø—ñ—Ç–∞–ª—É.\n\n"
        f"‚ùå **–í–Ü–î–ú–û–í–ê –í–Ü–î –í–Ü–î–ü–û–í–Ü–î–ê–õ–¨–ù–û–°–¢–Ü:** *–¶–µ –Ω–µ —Ñ—ñ–Ω–∞–Ω—Å–æ–≤–∞ –ø–æ—Ä–∞–¥–∞. "
        f"–ü–æ—Ç–µ–Ω—Ü—ñ–π–Ω—ñ {simulated_return}% —î —Å–∏–º—É–ª—è—Ü—ñ—î—é. –Ü–Ω–≤–µ—Å—Ç—É–≤–∞–Ω–Ω—è "
        f"–ø–æ–≤'—è–∑–∞–Ω–µ –∑ –≤–∏—Å–æ–∫–∏–º–∏ —Ä–∏–∑–∏–∫–∞–º–∏.*"
    )

def calculate_liquidity_risk() -> Dict[str, Any]:
    days_on_olx = random.randint(1, 100)
    risk_score = min(99, int(days_on_olx * 0.8))
    
    if days_on_olx > 60:
        status = "–í–∏—Å–æ–∫–∏–π (–¢—Ä–∏–≤–∞–ª–∏–π —á–∞—Å –Ω–∞ —Ä–∏–Ω–∫—É)"
    elif days_on_olx > 20:
        status = "–°–µ—Ä–µ–¥–Ω—ñ–π (–£ –º–µ–∂–∞—Ö –Ω–æ—Ä–º–∏)"
    else:
        status = "–ù–∏–∑—å–∫–∏–π (–°–≤—ñ–∂–µ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è)"
        
    return {
        "days_on_olx": days_on_olx,
        "risk_score": risk_score,
        "status": status
    }

async def _get_feedback_correction_factor(pool: asyncpg.Pool, olx_id: str) -> float:
    async with pool.acquire() as conn:
        record = await conn.fetchrow("""
            SELECT 
                SUM(CASE WHEN is_like = TRUE THEN 1 ELSE 0 END) AS likes,
                SUM(CASE WHEN is_like = FALSE THEN 1 ELSE 0 END) AS dislikes
            FROM user_feedback
            WHERE olx_id = $1
        """, olx_id)
        
        if record and (record['likes'] + record['dislikes']) > 0:
            likes = record['likes'] or 0
            dislikes = record['dislikes'] or 0
            total_feedback = likes + dislikes
            score = (likes - dislikes) / total_feedback
            return 1.0 + (score * EconomicConfig.FEEDBACK_CORRECTION_MULTIPLIER)
        
        return 1.0 


def _simulate_olx_post(query: str) -> Dict[str, Any]:
    base_id = datetime.now(KYIV_TZ).timestamp() + random.random()
    price = random.randint(25000, 150000)
    
    if 'rolex' in query.lower() or '–≥–æ–¥–∏–Ω–Ω–∏–∫' in query.lower():
        title = f"–ì–æ–¥–∏–Ω–Ω–∏–∫ Rolex Submariner (–°–ò–ú–£–õ–Ø–¶–Ü–Ø, ID: {int(base_id) % 100})"
        image = "https://i.imgur.com/example-rolex.jpg" 
        price = random.randint(80000, 200000)
    elif '–≤–µ—Ä—Å—Ç–∞—Ç' in query.lower() or '—á–ø—É' in query.lower():
        title = f"–ü—Ä–æ–º–∏—Å–ª–æ–≤–∏–π –ß–ü–£-–í–µ—Ä—Å—Ç–∞—Ç Haas (–°–ò–ú–£–õ–Ø–¶–Ü–Ø, ID: {int(base_id) % 100})"
        price = random.randint(150000, 500000)
        image = "https://i.imgur.com/example-cnc.jpg" 
    elif '–∑–æ–ª–æ—Ç–æ' in query.lower() or '—Å—Ä—ñ–±–ª–æ' in query.lower():
        title = f"–ó–æ–ª–æ—Ç–∏–π –ó–ª–∏—Ç–æ–∫ 585 / –ú–æ–Ω–µ—Ç–∞ (–°–ò–ú–£–õ–Ø–¶–Ü–Ø, ID: {int(base_id) % 100})"
        image = "https://i.imgur.com/example-gold.jpg"
    else:
        title = f"–¶—ñ–Ω–Ω–∏–π –ö–æ–ª–µ–∫—Ü—ñ–π–Ω–∏–π –ê–∫—Ç–∏–≤ '{query}' (–°–ò–ú–£–õ–Ø–¶–Ü–Ø, ID: {int(base_id) % 100})"
        image = "https://i.imgur.com/example-asset.jpg" 
        
    return {
        'olx_id': f"SIM_{int(base_id * 1000)}", 
        'title': title,
        'price': price,
        'url': "https://www.olx.ua/simulated-post",
        'image_url': image
    }

class EconomicEngine:
    def __init__(self, pool: asyncpg.Pool):
        self.pool = pool
        self.current_year = datetime.now(KYIV_TZ).year

    async def _fetch_external_auction_data(self, refined_keywords):
        keyword_seed = hash("".join(refined_keywords)) % 10000
        random.seed(keyword_seed) 
        
        is_machinery_context = any(word in "".join(refined_keywords).lower() for word in ['–≤–µ—Ä—Å—Ç–∞—Ç', '—á–ø—É', '–ø—Ä–µ—Å'])
        
        if is_machinery_context:
            avg_sale_price = random.randint(50000, 300000) 
            market_depth = random.randint(5, 50)
        else:
            avg_sale_price = random.randint(25000, 150000)
            market_depth = random.randint(1, 25)

        rarity_score = random.randint(EconomicConfig.MIN_RARITY_SCORE, 99)
        
        return {
            "source": "BCA Auction Data Simulation",
            "rarity_score": rarity_score,
            "average_sale_price": avg_sale_price,
            "market_depth": market_depth 
        }

    async def _analyze_machinery(self, title, olx_id, olx_price, ai_result):
        machinery_details = ai_result.get('machinery_details', {})
        refined_keywords = ai_result.get('refined_keywords', [title])
        external_data = await self._fetch_external_auction_data(refined_keywords)
        avg_sale_price = external_data.get('average_sale_price', olx_price * random.uniform(1.2, 2.0)) 
        
        feedback_multiplier = await _get_feedback_correction_factor(self.pool, olx_id)
        
        year = machinery_details.get('year_of_manufacture', self.current_year - 5)
        condition = machinery_details.get('condition_rating', 5)
        hours = machinery_details.get('operating_hours', 3000)
        
        age = max(0, self.current_year - year)
        depreciation_factor = min(0.9, age * EconomicConfig.MACHINERY_DEPRECIATION_RATE)
        value_after_age = avg_sale_price * (1 - depreciation_factor)

        condition_multiplier = (condition / 10) * EconomicConfig.MACHINERY_CONDITION_WEIGHT + (1 - EconomicConfig.MACHINERY_CONDITION_WEIGHT)
        hours_penalty_uah = hours * EconomicConfig.MACHINERY_HOURS_PENALTY_RATE * avg_sale_price 
        
        tiv_value_raw = round(value_after_age * condition_multiplier - hours_penalty_uah)
        
        tiv_value = round(tiv_value_raw * feedback_multiplier)
        tiv_value = max(EconomicConfig.OLX_PRICE_FILTER, tiv_value) 

        potential_profit_uah = round(tiv_value - olx_price - (olx_price * EconomicConfig.TRANSACTION_FEES_PERCENT / 100))
        potential_profit_percent = (potential_profit_uah / olx_price) * 100 if olx_price > 0 else 0
        
        is_relevant = potential_profit_percent > EconomicConfig.MIN_PROFIT_MARGIN_PERCENT
        deal_assessment = f"üî• –í–ò–ì–Ü–î–ù–ê –£–ì–û–î–ê ({potential_profit_percent:.1f}% –º–∞—Ä–∂–∞)" if is_relevant else f"–†–∏–Ω–∫–æ–≤–∞ –¶—ñ–Ω–∞ ({potential_profit_percent:.1f}% –º–∞—Ä–∂–∞)"
        
        liquidity_data = calculate_liquidity_risk()
            
        final_result = ai_result.copy()
        final_result.update({
            "is_relevant": is_relevant, "type": "–ü–†–û–§–ï–°–Ü–ô–ù–ï –û–ë–õ–ê–î–ù–ê–ù–ù–Ø (TIV Model)",
            "estimated_value": tiv_value, "deal_assessment": deal_assessment,
            "potential_profit_uah": potential_profit_uah, "risk_adjusted_value": tiv_value,
            "market_data": external_data, "liquidity_risk": liquidity_data,
            "feedback_multiplier": feedback_multiplier 
        })
        return final_result

    def _analyze_spot(self, olx_price, ai_result, spot_prices):
        is_gold = any(word in ai_result.get('refined_keywords', []) for word in ['–∑–æ–ª–æ—Ç–æ', '585'])
        
        if is_gold:
            spot_price_per_gram = spot_prices['GOLD_585_UAH_PER_GRAM']
            metal_type = "–ó–æ–ª–æ—Ç–æ 585"
        else: 
            spot_price_per_gram = spot_prices['SILVER_925_UAH_PER_GRAM']
            metal_type = "–°—Ä—ñ–±–ª–æ 925"

        estimated_weight_raw = ai_result.get('estimated_weight', 50) 
        
        implied_spot_value = estimated_weight_raw * spot_price_per_gram
        if olx_price > implied_spot_value * 5: 
            estimated_weight_g = max(1, estimated_weight_raw / 10) 
        else:
            estimated_weight_g = estimated_weight_raw

        calculated_spot_value = round(estimated_weight_g * spot_price_per_gram)
        
        premium_discount_percent = ((olx_price - calculated_spot_value) / calculated_spot_value) * 100 if calculated_spot_value > 0 else 100
        
        if premium_discount_percent < -1 * EconomicConfig.MIN_PROFIT_MARGIN_PERCENT:
            is_relevant = True
            deal_assessment = f"‚úÖ –ó–ù–ò–ñ–ö–ê {abs(premium_discount_percent):.1f}% (–ù–∏–∂—á–µ Spot Value)"
        else:
            is_relevant = False
            deal_assessment = f"‚ùå –ü–†–ï–ú–Ü–Ø {premium_discount_percent:.1f}% (–í–∏—â–µ Spot Value)"
            
        liquidity_data = calculate_liquidity_risk()

        final_result = ai_result.copy()
        final_result.update({
            "is_relevant": is_relevant, "type": f"–ú–ï–¢–ê–õ (SPOT: {metal_type})",
            "estimated_weight_g": estimated_weight_g, "spot_price_per_gram": spot_price_per_gram,
            "calculated_spot_value": calculated_spot_value, "premium_discount_percent": premium_discount_percent,
            "deal_assessment": deal_assessment, "liquidity_risk": liquidity_data
        })
        return final_result

    async def _analyze_collectible(self, title, olx_id, olx_price, ai_result):
        refined_keywords = ai_result.get('refined_keywords', [title])
        external_data = await self._fetch_external_auction_data(refined_keywords)
        
        feedback_multiplier = await _get_feedback_correction_factor(self.pool, olx_id)
        
        ai_rarity = ai_result.get('ai_rarity_score', external_data.get('rarity_score', 50))
        watch_details = ai_result.get('watch_details', {})
        authenticity_risk = watch_details.get('authenticity_risk_percent', 0)
        condition_rating = watch_details.get('condition_rating', 8)
        
        avg_auction_price = external_data.get('average_sale_price', olx_price * random.uniform(1.5, 3.0)) 
        
        rarity_factor = (ai_rarity / 100)
        condition_factor = condition_rating / 10
        authenticity_penalty = (1 - authenticity_risk / 100)

        prestige_multiplier = 1.0
        for brand, mult in EconomicConfig.PRESTIGE_MULTIPLIERS.items():
            if brand in title.lower():
                prestige_multiplier = mult
                break
        
        rav_value_raw = round(avg_auction_price * rarity_factor * condition_factor * authenticity_penalty * prestige_multiplier)
        
        rav_value = round(rav_value_raw * feedback_multiplier)

        potential_profit_uah = round(rav_value - olx_price - (rav_value * EconomicConfig.TRANSACTION_FEES_PERCENT / 100))
        potential_profit_percent = (potential_profit_uah / olx_price) * 100 if olx_price > 0 else 0

        is_relevant = potential_profit_percent > EconomicConfig.MIN_PROFIT_MARGIN_PERCENT
        deal_assessment = f"üî• –í–ò–ì–Ü–î–ù–ê –£–ì–û–î–ê ({potential_profit_percent:.1f}% –º–∞—Ä–∂–∞)" if is_relevant else f"–†–∏–Ω–∫–æ–≤–∞ –¶—ñ–Ω–∞ ({potential_profit_percent:.1f}% –º–∞—Ä–∂–∞)"
        
        liquidity_data = calculate_liquidity_risk()

        final_result = ai_result.copy()
        final_result.update({
            "is_relevant": is_relevant, "type": "–ö–û–õ–ï–ö–¶–Ü–ô–ù–ò–ô –ü–†–ï–î–ú–ï–¢ (RAV Model)",
            "estimated_value": rav_value, "deal_assessment": deal_assessment,
            "potential_profit_uah": potential_profit_uah, "risk_adjusted_value": rav_value,
            "market_data": external_data, "liquidity_risk": liquidity_data,
            "feedback_multiplier": feedback_multiplier 
        })
        return final_result


    async def analyze_olx_item(self, session, item, spot_prices, bot: Optional[Bot]=None):
        olx_id, title, olx_price, image_url = item['olx_id'], item['title'], item['price'], item['image_url']
        
        if olx_id.startswith('SIM_'):
             logger.info(f"–ê–Ω–∞–ª—ñ–∑ —Å–∏–º—É–ª—å–æ–≤–∞–Ω–æ–≥–æ –ø–æ—Å—Ç—É: {title}")
             if 'rolex' in title.lower() or '–≥–æ–¥–∏–Ω–Ω–∏–∫' in title.lower():
                 ai_vision_result = {'is_correct_type': True, 'refined_keywords': [title], 'ai_rarity_score': 85, 'watch_details': {'brand': 'Rolex', 'condition_rating': 9, 'authenticity_risk_percent': 10}}
             elif '—á–ø—É' in title.lower() or '–≤–µ—Ä—Å—Ç–∞—Ç' in title.lower():
                 ai_vision_result = {'is_correct_type': True, 'refined_keywords': [title], 'ai_rarity_score': 60, 'machinery_details': {'year_of_manufacture': 2020, 'operating_hours': 1500, 'condition_rating': 9}}
             elif '–∑–æ–ª–æ—Ç–∏–π' in title.lower() or '—Å—Ä—ñ–±–ª–æ' in title.lower():
                 ai_vision_result = {'is_correct_type': True, 'refined_keywords': [title], 'ai_rarity_score': 50, 'estimated_weight': 20}
             else:
                  ai_vision_result = {'is_correct_type': True, 'refined_keywords': [title], 'ai_rarity_score': 70}

        else:
             base64_image = await get_image_base64(session, image_url)
             
             analysis_schema = {
                 "type": "OBJECT", "properties": { 
                     "is_correct_type": {"type": "BOOLEAN"},"refined_keywords": {"type": "ARRAY", "items": {"type": "STRING"}},
                     "ai_rarity_score": {"type": "INTEGER"},"estimated_weight": {"type": "NUMBER"},
                     "watch_details": {"type": "OBJECT", "properties": {"brand": {"type": "STRING"}, "model": {"type": "STRING"}, "condition_rating": {"type": "INTEGER"}, "authenticity_risk_percent": {"type": "INTEGER"}}},
                     "machinery_details": {"type": "OBJECT", "properties": {"manufacturer": {"type": "STRING"}, "model": {"type": "STRING"}, "year_of_manufacture": {"type": "INTEGER"}, "operating_hours": {"type": "INTEGER"}, "condition_rating": {"type": "INTEGER"}}}
                 }, "required": ["is_correct_type", "refined_keywords", "ai_rarity_score"]
             }
             
             ai_vision_result = {}
             if base64_image:
                 prompt = f"–ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π—Ç–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è. –ó–∞–≥–æ–ª–æ–≤–æ–∫: '{title}'. –¶—ñ–Ω–∞: {olx_price} UAH."
                 ai_vision_result = await gemini_vision_analysis(
                     session, prompt, base64_image, self.pool, ADMIN_ID, analysis_schema 
                 ) or {}
            
        if not ai_vision_result.get('is_correct_type', True):
            return {"is_relevant": False, "type": "–í—ñ–¥—Ö–∏–ª–µ–Ω–æ AI Vision", "deal_assessment": "–ù–µ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó"}
            
        is_machinery = any(word in title.lower() for word in ['–≤–µ—Ä—Å—Ç–∞—Ç', '—á–ø—É', '–ø—Ä–µ—Å', '—ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç'])
        is_scrap_or_bullion = any(keyword in title.lower() for keyword in EconomicConfig.OLX_SCRAP_QUERIES)

        if is_machinery or ai_vision_result.get('machinery_details', {}).get('manufacturer'):
            return await self._analyze_machinery(title, olx_id, olx_price, ai_vision_result)
        
        elif is_scrap_or_bullion or ai_vision_result.get('estimated_weight'):
            return self._analyze_spot(olx_price, ai_vision_result, spot_prices)

        else: 
            return await self._analyze_collectible(title, olx_id, olx_price, ai_vision_result)


async def _fetch_single_query(session, pool, search_term, existing_ids):
    posts_for_query = []
    olx_search_url = f"https://www.olx.ua/d/uk/list/q-{search_term}/?currency=UAH&search%5Bfilter_float_price%3Afrom%5D={EconomicConfig.OLX_PRICE_FILTER}"
    
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}
        async with session.get(olx_search_url, headers=headers, timeout=20) as response:
            
            if response.status == 403: 
                logger.error(f"–ü–æ–º–∏–ª–∫–∞ HTTP 403 (Forbidden) –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ OLX ({search_term}). –ô–º–æ–≤—ñ—Ä–Ω–æ, OLX –∑–∞–±–ª–æ–∫—É–≤–∞–≤ –∑–∞–ø–∏—Ç.")
                return posts_for_query 
            
            response.raise_for_status()
            html = await response.text()
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ OLX ({search_term}): {e}")
        return posts_for_query

    soup = BeautifulSoup(html, 'lxml')
    items = soup.find_all('div', {'data-cy': re.compile(r'l-card')})

    for item in items:
        olx_url_tag = item.find('a', {'data-cy': 'listing-ad-link'})
        if not olx_url_tag: continue
            
        full_url = urljoin(olx_search_url, olx_url_tag.get('href'))
        match = re.search(r'-ID(\d+)\.html', full_url)
        olx_id = match.group(1) if match else None
        
        if not olx_id or olx_id in existing_ids: continue

        title = item.find('h6').text.strip() if item.find('h6') else 'N/A'
        price_text = item.find('p', {'data-testid': 'price'}).text.strip() if item.find('p', {'data-testid': 'price'}) else '0 UAH'
        price_match = re.search(r'([\d\s]+)', price_text)
        price_uah = int("".join(price_match.group(1).split())) if price_match else 0
        
        img_tag = item.find('img')
        image_url = img_tag.get('src') if img_tag and 'src' in img_tag.attrs else None
        
        if price_uah < EconomicConfig.OLX_PRICE_FILTER: continue

        posts_for_query.append({
            'olx_id': olx_id,
            'title': title,
            'price': price_uah,
            'url': full_url,
            'image_url': image_url
        })
        
    return posts_for_query

async def fetch_olx_data(session, pool):
    all_new_posts = []
    
    async with pool.acquire() as conn:
        existing_ids = await conn.fetchval("SELECT array_agg(olx_id) FROM olx_posts") or []

    search_queries = EconomicConfig.OLX_SEARCH_QUERIES + EconomicConfig.OLX_SCRAP_QUERIES
    
    tasks = [_fetch_single_query(session, pool, term, existing_ids) for term in search_queries]
    results = await asyncio.gather(*tasks)

    for posts in results:
        all_new_posts.extend(posts)
        
    unique_posts = list({post['olx_id']: post for post in all_new_posts}.values())
    
    if not unique_posts:
        logger.warning("OLX —Å–∫—Ä–µ–π–ø—ñ–Ω–≥ –Ω–µ –ø—Ä–∏–Ω—ñ—Å —Ä–µ–∞–ª—å–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤. –ê–ö–¢–ò–í–û–í–ê–ù–û –†–ï–ñ–ò–ú –°–ò–ú–£–õ–Ø–¶–Ü–á (5 –ø–æ—Å—Ç—ñ–≤).")
        simulated_queries = random.sample(search_queries, min(5, len(search_queries)))
        for query in simulated_queries:
             sim_post = _simulate_olx_post(query)
             if sim_post['olx_id'] not in existing_ids and sim_post['olx_id'] not in [p['olx_id'] for p in unique_posts]:
                 unique_posts.append(sim_post)
        
    logger.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(unique_posts)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –Ω–æ–≤–∏—Ö –æ–≥–æ–ª–æ—à–µ–Ω—å –¥–ª—è –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É (–≤–∫–ª—é—á–∞—é—á–∏ —Å–∏–º—É–ª—è—Ü—ñ—é).")
    return unique_posts

def get_feedback_keyboard(olx_id):
    return InlineKeyboardMarkup(inline_keyboard=[
        [
            InlineKeyboardButton(text="üëç –õ–∞–π–∫ (–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ)", callback_data=f"fb_like_{olx_id}"),
            InlineKeyboardButton(text="üëé –ù–µ –õ–∞–π–∫ (–ù–µ–≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ—Å—Ç—å)", callback_data=f"fb_dislike_{olx_id}")
        ]
    ])

def get_learning_keyboard():
    return InlineKeyboardMarkup(inline_keyboard=[
        [
            InlineKeyboardButton(text="üîÅ –ù–∞—Å—Ç—É–ø–Ω–∏–π –£—Ä–æ–∫", callback_data="learn_next"),
            InlineKeyboardButton(text="üö´ –ó–º—ñ–Ω–∏—Ç–∏ –¢–µ–º—É", callback_data="learn_change_topic")
        ]
    ])

async def send_olx_post(bot: Bot, item: Dict[str, Any], ai_result: Dict[str, Any]):
    olx_price_formatted = f"{item['price']:,}".replace(',', ' ')
    deal_assessment_text = ai_result.get('deal_assessment', '–ù/–î')
    
    post_type = ai_result.get('type', '–ù/–î')
    is_machinery_report = "–û–ë–õ–ê–î–ù–ê–ù–ù–Ø" in post_type
    is_spot_report = "SPOT" in post_type
    
    liquidity_data = ai_result.get('liquidity_risk', {})
    
    refined_keys = ", ".join(ai_result.get('refined_keywords', ['–ù/–î']))
    ai_rarity_score = ai_result.get('ai_rarity_score', '–ù/–î')
    
    fb_multiplier = ai_result.get('feedback_multiplier', 1.0)
    fb_text = f"**x{fb_multiplier:.2f}**"
    if fb_multiplier > 1.05:
        fb_text = f"**‚¨ÜÔ∏è {fb_text} (–ü—ñ–¥–≤–∏—â–µ–Ω–æ)**"
    elif fb_multiplier < 0.95:
        fb_text = f"**‚¨áÔ∏è {fb_text} (–ó–Ω–∏–∂–µ–Ω–æ)**"
    else:
        fb_text = f"**{fb_text} (–ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ)**"
    
    header = f"[{post_type.split('(')[0].strip()}] {item['title']}\n"
    if item['olx_id'].startswith('SIM_'):
        header += "‚ö†Ô∏è **–¶–µ —Å–∏–º—É–ª—å–æ–≤–∞–Ω–∏–π –ø–æ—Å—Ç (OLX –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π)**\n"
        
    header += f"**üö® –û–¶–Ü–ù–ö–ê –í–ò–ì–û–î–ò:** **{deal_assessment_text}**\n"
    header += f"üí∞ **–¶—ñ–Ω–∞ OLX:** *{olx_price_formatted} UAH*\n"
    header += f"---------------------------------------\n"
    
    core_metrics = ""
    
    if is_machinery_report:
        machinery_details = ai_result.get('machinery_details', {})
        tiv_formatted = f"{ai_result.get('risk_adjusted_value', 0):,}".replace(',', ' ')
        profit_uah_formatted = f"{ai_result.get('potential_profit_uah', 0):,}".replace(',', ' ')
        
        core_metrics = (
            f"**‚öôÔ∏è –¢–ï–•–ù–Ü–ß–ù–Ü –î–ê–ù–Ü (AI Vision)**\n"
            f"**–í–∏—Ä–æ–±–Ω–∏–∫/–ú–æ–¥–µ–ª—å:** `{machinery_details.get('manufacturer', '–ù/–î')} / {machinery_details.get('model', '–ù/–î')}`\n"
            f"**–†—ñ–∫/–ù–∞–ø—Ä–∞—Ü—é–≤–∞–Ω–Ω—è:** `{machinery_details.get('year_of_manufacture', '–ù/–î')} / {machinery_details.get('operating_hours', '–ù/–î')} –≥–æ–¥.`\n"
            f"**–í—ñ–∑—É–∞–ª—å–Ω–∏–π –°—Ç–∞–Ω (1-10):** `{machinery_details.get('condition_rating', '–ù/–î')}/10`\n"
            f"---------------------------------------\n"
            f"**üìà –§–Ü–ù–ê–ù–°–û–í–ò–ô –ê–ù–ê–õ–Ü–ó (TIV)**\n"
            f"**TIV (–Ü–Ω–≤. –í–∞—Ä—Ç—ñ—Å—Ç—å):** `{tiv_formatted} UAH`\n"
            f"**–ü—Ä–æ–≥–Ω–æ–∑–æ–≤–∞–Ω–∏–π –ü—Ä–∏–±—É—Ç–æ–∫:** `{profit_uah_formatted} UAH`\n"
        )
    
    elif is_spot_report:
        calculated_spot_value_formatted = f"{ai_result.get('calculated_spot_value', 0):,}".replace(',', ' ')
        premium_discount = ai_result.get('premium_discount_percent', 0)
        spot_price_per_gram = ai_result.get('spot_price_per_gram', '–ù/–î')
        estimated_weight_g = ai_result.get('estimated_weight_g', '–ù/–î')

        core_metrics = (
            f"**‚öñÔ∏è –ú–ï–¢–ê–õ–ï–í–ò–ô –ê–ù–ê–õ–Ü–ó (SPOT)**\n"
            f"**–û—Ü—ñ–Ω–æ—á–Ω–∞ –í–∞–≥–∞ (AI):** `{estimated_weight_g:.2f} –≥`\n"
            f"**–ü–æ—Ç–æ—á–Ω–∞ Spot –¶—ñ–Ω–∞:** `{spot_price_per_gram:.2f} UAH/–≥`\n"
            f"**–°–ø—Ä–∞–≤–µ–¥–ª–∏–≤–∞ Spot Value:** `{calculated_spot_value_formatted} UAH`\n"
            f"**–ü—Ä–µ–º—ñ—è/–ó–Ω–∏–∂–∫–∞:** `{premium_discount:.1f}%`\n"
        )

    else:
        rav_formatted = f"{ai_result.get('risk_adjusted_value', 0):,}".replace(',', ' ')
        profit_uah_formatted = f"{ai_result.get('potential_profit_uah', 0):,}".replace(',', ' ')
        market_depth = ai_result.get('market_data', {}).get('market_depth', '–ù/–î')

        core_metrics = (
            f"üñºÔ∏è **–ö–û–õ–ï–ö–¶–Ü–ô–ù–ò–ô –ê–ù–ê–õ–Ü–ó (RAV)**\n"
            f"**RAV (–†–∏–∑–∏–∫–æ–≤–∞–Ω–∞ –í–∞—Ä—Ç—ñ—Å—Ç—å):** `{rav_formatted} UAH`\n"
            f"**–ü—Ä–æ–≥–Ω–æ–∑–æ–≤–∞–Ω–∏–π –ü—Ä–∏–±—É—Ç–æ–∫:** `{profit_uah_formatted} UAH`\n"
            f"**Rarity Score (AI/BCA):** `{ai_rarity_score}`\n"
            f"**–ì–ª–∏–±–∏–Ω–∞ –†–∏–Ω–∫—É (BCA):** `{market_depth} –ø—Ä–æ–¥–∞–∂—ñ–≤`\n"
        )
        
    footer = (
        f"---------------------------------------\n"
        f"**üß† AI KEYWORDS:** *{refined_keys}*\n"
        f"**ü§ñ –ö–û–ï–§. –ù–ê–í–ß–ê–ù–ù–Ø (FB):** {fb_text}\n" 
        f"**üìâ –†–ò–ó–ò–ö –õ–Ü–ö–í–Ü–î–ù–û–°–¢–Ü:** *{liquidity_data.get('status', '–ù/–î')}* (ID: {liquidity_data.get('days_on_olx', '–ù/–î')} –¥–Ω.)\n"
        f"[‚û°Ô∏è –ü–µ—Ä–µ–π—Ç–∏ –¥–æ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è]({item['url']})"
    )

    message_text = header + core_metrics + footer

    try:
        image_to_send = item['image_url'] if not item['olx_id'].startswith('SIM_') else "https://i.imgur.com/example-asset.jpg" 

        if image_to_send:
            await bot.send_photo(
                chat_id=CHANNEL_ID,
                photo=image_to_send,
                caption=message_text,
                parse_mode=ParseMode.MARKDOWN,
                reply_markup=get_feedback_keyboard(item['olx_id'])
            )
        else:
            await bot.send_message(
                chat_id=CHANNEL_ID,
                text=message_text,
                parse_mode=ParseMode.MARKDOWN,
                reply_markup=get_feedback_keyboard(item['olx_id'])
            )
        return True
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –≤—ñ–¥–ø—Ä–∞–≤—Ü—ñ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –≤ Telegram: {e}")
        return False

dp = Dispatcher()

@dp.message(Command("start"))
async def command_start_handler(message: types.Message, conn: asyncpg.Connection):
    username = message.from_user.username or message.from_user.full_name
    try:
        await conn.execute('INSERT INTO users (user_id, username, joined_at) VALUES ($1, $2, $3) ON CONFLICT (user_id) DO UPDATE SET username = $2',
                           message.from_user.id, username, datetime.now(KYIV_TZ))
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –¥–æ–¥–∞–≤–∞–Ω–Ω—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞: {e}")

    welcome_message = (
        "üíé **–õ–∞—Å–∫–∞–≤–æ –ø—Ä–æ—Å–∏–º–æ –Ω–∞ –ü—Ä–æ—Ñ–µ—Å—ñ–π–Ω—É –ê–Ω–∞–ª—ñ—Ç–∏—á–Ω—É –ü–ª–∞—Ç—Ñ–æ—Ä–º—É V10!** üõ†Ô∏è\n\n"
        "–Ø - –í–∞—à AI-–ø–æ–º—ñ—á–Ω–∏–∫ –¥–ª—è –ø–æ—à—É–∫—É –≤–∏—Å–æ–∫–æ—Ü—ñ–Ω–Ω–∏—Ö –∞–∫—Ç–∏–≤—ñ–≤ —Ç–∞ —ñ–Ω–≤–µ—Å—Ç–∏—Ü—ñ–π–Ω–∏—Ö —É–≥–æ–¥.\n"
        "**–°–∏—Å—Ç–µ–º–∞ V10 –ê–¥–∞–ø—Ç–∏–≤–Ω–æ –ù–∞–≤—á–∞—î—Ç—å—Å—è** –Ω–∞ –í–∞—à–æ–º—É –∑–≤–æ—Ä–æ—Ç–Ω—å–æ–º—É –∑–≤'—è–∑–∫—É —Ç–∞ –µ—Ç–∞–ª–æ–Ω–∞—Ö –ë–∞–∑–∏ –ó–Ω–∞–Ω—å.\n\n"
        "‚Ä¢ **TIV Model:** –ê–Ω–∞–ª—ñ–∑ –ø—Ä–æ–º–∏—Å–ª–æ–≤–æ–≥–æ –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è.\n"
        "‚Ä¢ **RAV Model:** –ê–Ω–∞–ª—ñ–∑ –∫–æ–ª–µ–∫—Ü—ñ–π–Ω–∏—Ö –ø—Ä–µ–¥–º–µ—Ç—ñ–≤ (–≥–æ–¥–∏–Ω–Ω–∏–∫–∏, –º–æ–Ω–µ—Ç–∏).\n\n"
        "**üìö –ù–ê–í–ß–ê–õ–¨–ù–ò–ô –ú–û–î–£–õ–¨ (CollectorLearning)**\n"
        "‚Ä¢ **/learn_collector:** –ü–æ—á–Ω—ñ—Ç—å —Å–≤—ñ–π —à–ª—è—Ö –¥–æ —Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è –µ–∫—Å–ø–µ—Ä—Ç–æ–º-–∫–æ–ª–µ–∫—Ü—ñ–æ–Ω–µ—Ä–æ–º!\n\n"
        "**‚öôÔ∏è –Ü–ù–®–Ü –ö–û–ú–ê–ù–î–ò**\n"
        "‚Ä¢ **/spot:** –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –ø–æ—Ç–æ—á–Ω—ñ —Ä–∏–Ω–∫–æ–≤—ñ —Ü—ñ–Ω–∏.\n"
        "‚Ä¢ **/settings:** –ü–µ—Ä–µ–≥–ª—è–Ω—É—Ç–∏ –ø–æ—Ç–æ—á–Ω—É –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é —Å–∏—Å—Ç–µ–º–∏.\n"
        "‚Ä¢ **/analyze_cutlery:** –ê–Ω–∞–ª—ñ–∑ –Ω–∞ –≤–º—ñ—Å—Ç —Å—Ä—ñ–±–ª–∞.\n"
        "‚Ä¢ **/base:** –î–æ–¥–∞—Ç–∏ –µ—Ç–∞–ª–æ–Ω–Ω–∏–π –∑—Ä–∞–∑–æ–∫ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è AI Vision."
    )
    await message.answer(welcome_message, parse_mode=ParseMode.MARKDOWN)

@dp.message(Command("proposals"))
async def command_proposals_handler(message: types.Message):
    proposal = generate_high_yield_proposal()
    await message.answer(proposal, parse_mode=ParseMode.MARKDOWN)

@dp.message(Command("spot"))
async def command_spot_handler(message: types.Message):
    spot_prices = EconomicConfig.SPOT_PRICES
    updated_at = datetime.fromtimestamp(spot_prices['LAST_UPDATED'], KYIV_TZ).strftime("%d.%m.%Y %H:%M")
    
    response = (
        "üìä **–ü–û–¢–û–ß–ù–Ü –†–ò–ù–ö–û–í–Ü SPOT –¶–Ü–ù–ò (–°–∏–º—É–ª—è—Ü—ñ—è)**\n\n"
        f"**ü•á –ó–æ–ª–æ—Ç–æ 585:** `{spot_prices['GOLD_585_UAH_PER_GRAM']:.2f} UAH/–≥—Ä–∞–º`\n"
        f"**ü•à –°—Ä—ñ–±–ª–æ 925:** `{spot_prices['SILVER_925_UAH_PER_GRAM']:.2f} UAH/–≥—Ä–∞–º`\n\n"
        f"*[–î–∞–Ω—ñ –æ–Ω–æ–≤–ª–µ–Ω–æ: {updated_at} (–ö–∏—ó–≤)]*"
    )
    await message.answer(response, parse_mode=ParseMode.MARKDOWN)

@dp.message(Command("settings"))
async def command_settings_handler(message: types.Message):
    config = EconomicConfig
    
    settings_text = (
        "‚öôÔ∏è **–ù–ê–õ–ê–®–¢–£–í–ê–ù–ù–Ø –ü–õ–ê–¢–§–û–†–ú–ò V10**\n\n"
        "**1. –ï–ö–û–ù–û–ú–Ü–ß–ù–Ü –ö–û–ù–°–¢–ê–ù–¢–ò**\n"
        f"‚Ä¢ –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –ú–∞—Ä–∂–∞: `{config.MIN_PROFIT_MARGIN_PERCENT}%`\n"
        f"‚Ä¢ –ö–æ–º—ñ—Å—ñ—è –ü–µ—Ä–µ–ø—Ä–æ–¥–∞–∂—É (–£–º–æ–≤–Ω–∞): `{config.TRANSACTION_FEES_PERCENT}%`\n"
        f"‚Ä¢ –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π –ü–æ—à—É–∫–æ–≤–∏–π –ü–æ—Ä—ñ–≥: `{config.OLX_PRICE_FILTER} UAH`\n\n"
        
        "**2. –ú–û–î–ï–õ–¨ TIV (–û–±–ª–∞–¥–Ω–∞–Ω–Ω—è)**\n"
        f"‚Ä¢ –©–æ—Ä—ñ—á–Ω–∞ –ê–º–æ—Ä—Ç–∏–∑–∞—Ü—ñ—è: `{config.MACHINERY_DEPRECIATION_RATE * 100}%`\n"
        f"‚Ä¢ –í–∞–≥–∞ –°—Ç–∞–Ω—É —É TIV: `{config.MACHINERY_CONDITION_WEIGHT * 100}%`\n\n"
        
        "**3. –ú–û–î–ï–õ–¨ RAV (–ö–æ–ª–µ–∫—Ü—ñ–π–Ω—ñ)**\n"
        f"‚Ä¢ –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π Rarity Score: `{config.MIN_RARITY_SCORE}`\n"
        f"‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π –†–∏–∑–∏–∫ –ê–≤—Ç–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—ñ: `{config.MAX_AUTHENTICITY_RISK}%`\n"
        f"‚Ä¢ **–í–ø–ª–∏–≤ –§—ñ–¥–±–µ–∫—É (–ö–æ—Ä–µ–∫—Ü—ñ—è):** `{config.FEEDBACK_CORRECTION_MULTIPLIER * 100}%`\n"
        f"‚Ä¢ –ú–Ω–æ–∂–Ω–∏–∫–∏ –ü—Ä–µ—Å—Ç–∏–∂—É: {', '.join([f'{k.capitalize()}: {v}' for k, v in config.PRESTIGE_MULTIPLIERS.items()])}\n\n"
        
        "**4. OLX –ú–û–ù–Ü–¢–û–†–ò–ù–ì (–ó–∞–ø–∏—Ç–∏)**\n"
        f"‚Ä¢ –ö–æ–ª–µ–∫—Ü—ñ–π–Ω—ñ/–¶—ñ–Ω–Ω—ñ: `{', '.join(config.OLX_SEARCH_QUERIES)}`\n"
        f"‚Ä¢ –õ–æ–º/–ë—É–ª—å–π–æ–Ω: `{', '.join(config.OLX_SCRAP_QUERIES)}`"
    )
    await message.answer(settings_text, parse_mode=ParseMode.MARKDOWN)

@dp.message(Command("admin_status"))
async def command_admin_status_handler(message: types.Message, pool: asyncpg.Pool):
    if message.from_user.id != ADMIN_ID:
        await message.answer("‚ùå **–í—ñ–¥–º–æ–≤–ª–µ–Ω–æ —É –¥–æ—Å—Ç—É–ø—ñ.** –í–∏ –Ω–µ —î –∞–¥–º—ñ–Ω—ñ—Å—Ç—Ä–∞—Ç–æ—Ä–æ–º.")
        return

    async with pool.acquire() as conn:
        total_posts = await conn.fetchval("SELECT COUNT(*) FROM olx_posts")
        relevant_posts = await conn.fetchval("SELECT COUNT(*) FROM olx_posts WHERE is_relevant = TRUE")
        
        feedback_like = await conn.fetchval("SELECT COUNT(*) FROM user_feedback WHERE is_like = TRUE")
        feedback_dislike = await conn.fetchval("SELECT COUNT(*) FROM user_feedback WHERE is_like = FALSE")
        
        total_users = await conn.fetchval("SELECT COUNT(*) FROM users")
        total_base_records = await conn.fetchval("SELECT COUNT(*) FROM user_base")
        
        avg_correction_factor = await conn.fetchval("""
            SELECT AVG( (ai_analysis_json->>'feedback_multiplier')::float ) 
            FROM olx_posts 
            WHERE ai_analysis_json ? 'feedback_multiplier'
        """)


    status_text = (
        "üëë **–ê–î–ú–Ü–ù-–°–¢–ê–¢–£–° –ü–õ–ê–¢–§–û–†–ú–ò V10**\n\n"
        "**üìä –ú–û–ù–Ü–¢–û–†–ò–ù–ì OLX/AI**\n"
        f"‚Ä¢ –í—Å—å–æ–≥–æ –ü—Ä–æ–∞–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–æ –ü–æ—Å—Ç—ñ–≤: `{total_posts}`\n"
        f"‚Ä¢ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö (–û–ø—É–±–ª—ñ–∫–æ–≤–∞–Ω–æ): `{relevant_posts}` ({relevant_posts/total_posts * 100 if total_posts else 0:.1f}%) \n"
        f"‚Ä¢ –°–µ—Ä. –ö–æ–µ—Ñ. –ù–∞–≤—á–∞–Ω–Ω—è (FB): `{avg_correction_factor:.3f}`\n\n"
        
        "**üëç –ó–í–û–†–û–¢–ù–ò–ô –ó–í'–Ø–ó–û–ö (–î–ª—è –ù–∞–≤—á–∞–Ω–Ω—è AI)**\n"
        f"‚Ä¢ –õ–∞–π–∫—ñ–≤ (–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å –ü—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–∞): `{feedback_like}`\n"
        f"‚Ä¢ –î–∏—Å–ª–∞–π–∫—ñ–≤ (–ù–µ–≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ—Å—Ç—å): `{feedback_dislike}`\n\n"
        
        "**üë§ –ö–û–†–ò–°–¢–£–í–ê–ß–Ü –¢–ê –ù–ê–í–ß–ê–ù–ù–Ø**\n"
        f"‚Ä¢ –í—Å—å–æ–≥–æ –ö–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤: `{total_users}`\n"
        f"‚Ä¢ –ó–∞–ø–∏—Å—ñ–≤ —É –ë–∞–∑—ñ –ó–Ω–∞–Ω—å: `{total_base_records}`"
    )
    await message.answer(status_text, parse_mode=ParseMode.MARKDOWN)

@dp.message(Command("learn_collector"))
async def command_start_learning(message: types.Message, state: FSMContext):
    await state.clear()
    await state.set_state(LearningState.waiting_for_topic)
    
    suggested_topics = ['–û—Ü—ñ–Ω–∫–∞ —Ä—ñ–¥–∫—ñ—Å–Ω–∏—Ö –∑–æ–ª–æ—Ç–∏—Ö –º–æ–Ω–µ—Ç', '–†–∏–∑–∏–∫–∏ –∞–≤—Ç–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—ñ –ø—Ä–µ–º—ñ–∞–ª—å–Ω–∏—Ö –≥–æ–¥–∏–Ω–Ω–∏–∫—ñ–≤', '–û—Ü—ñ–Ω–∫–∞ –∑–Ω–æ—Å—É –ø—Ä–æ–º–∏—Å–ª–æ–≤–æ–≥–æ –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è']
    
    await message.answer(
        "üìö **–ö–û–õ–ï–ö–¶–Ü–û–ù–ï–†-–ï–ö–°–ü–ï–†–¢ (–ö—Ä–æ–∫ 1/3)**\n\n"
        "–ù–∞–ø–∏—à—ñ—Ç—å, –ø—Ä–æ —è–∫—É —Ç–µ–º—É —ñ–Ω–≤–µ—Å—Ç–∏—Ü—ñ–π–Ω–æ–≥–æ –∫–æ–ª–µ–∫—Ü—ñ–æ–Ω—É–≤–∞–Ω–Ω—è –í–∏ –± —Ö–æ—Ç—ñ–ª–∏ –æ—Ç—Ä–∏–º–∞—Ç–∏ –¥–µ—Ç–∞–ª—å–Ω–∏–π —É—Ä–æ–∫ —Ç–∞ –≤—ñ–∫—Ç–æ—Ä–∏–Ω—É:\n"
        f"–ù–∞–ø—Ä–∏–∫–ª–∞–¥: `{suggested_topics[0]}`, `{suggested_topics[1]}` –∞–±–æ `{suggested_topics[2]}`."
    )

@dp.message(LearningState.waiting_for_topic, F.text)
async def process_learning_topic(message: types.Message, state: FSMContext, session: aiohttp.ClientSession, pool: asyncpg.Pool):
    topic = message.text.strip()
    await message.answer(f"‚è≥ –ó–∞–ø—É—Å–∫–∞—é AI-–∫—É—Ä–∞—Ç–æ—Ä–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —É—Ä–æ–∫—É –ø–æ —Ç–µ–º—ñ: **{topic}** (–∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º —Å–ø—ñ—Ä–Ω–∏—Ö –∞–∫—Ç–∏–≤—ñ–≤)...", parse_mode=ParseMode.MARKDOWN)
    
    lesson_data = await generate_collector_lesson(session, topic, pool, message.from_user.id)
    
    if not lesson_data:
        await message.answer("‚ùå **–ü–æ–º–∏–ª–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —É—Ä–æ–∫—É.** –°–ø—Ä–æ–±—É–π—Ç–µ –∑–º—ñ–Ω–∏—Ç–∏ —Ç–µ–º—É –∞–±–æ –ø–æ–≤—Ç–æ—Ä—ñ—Ç—å –ø—ñ–∑–Ω—ñ—à–µ.")
        await state.clear()
        return

    await state.update_data(
        topic=topic,
        quiz_answer=lesson_data['quiz_answer'],
        quiz_hint=lesson_data['quiz_hint']
    )
    
    lesson_message = (
        f"üéì **–£–†–û–ö: {lesson_data['lesson_title']}**\n\n"
        f"{lesson_data['content']}\n\n"
        f"---------------------------------------\n"
        f"‚ùì **–í–Ü–ö–¢–û–†–ò–ù–ê:**\n"
        f"*{lesson_data['quiz_question']}* \n\n"
        f"–ù–∞–¥—ñ—à–ª—ñ—Ç—å –í–∞—à—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å, —â–æ–± –ø–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –∑–Ω–∞–Ω–Ω—è."
    )
    await message.answer(lesson_message, parse_mode=ParseMode.MARKDOWN)
    await state.set_state(LearningState.in_session)


@dp.message(LearningState.in_session, F.text)
async def process_quiz_answer(message: types.Message, state: FSMContext):
    user_answer = message.text.strip().lower()
    data = await state.get_data()
    
    correct_answer = data['quiz_answer'].strip().lower()
    quiz_hint = data['quiz_hint']
    
    if correct_answer in user_answer or user_answer in correct_answer or user_answer == "—Ç–∞–∫":
        feedback = (
            f"‚úÖ **–ü–†–ê–í–ò–õ–¨–ù–û!** –í–∏ —á—É–¥–æ–≤–æ –∑–∞—Å–≤–æ—ó–ª–∏ –º–∞—Ç–µ—Ä—ñ–∞–ª –ø–æ —Ç–µ–º—ñ **{data['topic']}**.\n"
            f"**–ü—Ä–∞–≤–∏–ª—å–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å:** *{data['quiz_answer']}*\n\n"
            f"–í–∏ –Ω–∞ –∫—Ä–æ–∫ –±–ª–∏–∂—á–µ –¥–æ –∑–≤–∞–Ω–Ω—è –µ–∫—Å–ø–µ—Ä—Ç–∞!"
        )
    else:
        feedback = (
            f"‚ùå **–ù–ï–ü–†–ê–í–ò–õ–¨–ù–û.** –ù–µ —Ö–≤–∏–ª—é–π—Ç–µ—Å—å, —Ü–µ —Å–∫–ª–∞–¥–Ω–∏–π –º–∞—Ç–µ—Ä—ñ–∞–ª.\n"
            f"**–ü—ñ–¥–∫–∞–∑–∫–∞:** *{quiz_hint}*\n"
            f"**–ü—Ä–∞–≤–∏–ª—å–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å:** *{data['quiz_answer']}*\n\n"
            f"–°–ø—Ä–æ–±—É–π—Ç–µ –∑–Ω–æ–≤—É –∞–±–æ –ø–µ—Ä–µ–π–¥—ñ—Ç—å –¥–æ –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —É—Ä–æ–∫—É."
        )
        
    await message.answer(feedback, parse_mode=ParseMode.MARKDOWN, reply_markup=get_learning_keyboard())
    await state.set_state(LearningState.waiting_for_next)

@dp.callback_query(LearningState.waiting_for_next, F.data.startswith('learn_'))
async def process_learning_callback(callback_query: types.CallbackQuery, state: FSMContext, pool: asyncpg.Pool, session: aiohttp.ClientSession):
    await callback_query.answer() 
    
    if callback_query.data == 'learn_next':
        data = await state.get_data()
        topic = data.get('topic')
        
        await callback_query.message.answer(f"‚è≥ –ì–µ–Ω–µ—Ä—É—é –Ω–∞—Å—Ç—É–ø–Ω–∏–π —É—Ä–æ–∫ –ø–æ —Ç–µ–º—ñ: **{topic}**...", parse_mode=ParseMode.MARKDOWN)
        
        lesson_data = await generate_collector_lesson(session, topic, pool, callback_query.from_user.id)
        
        if not lesson_data:
            await callback_query.message.answer("‚ùå **–ü–æ–º–∏–ª–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —É—Ä–æ–∫—É.** –°–ø—Ä–æ–±—É–π—Ç–µ –∑–º—ñ–Ω–∏—Ç–∏ —Ç–µ–º—É –∞–±–æ –ø–æ–≤—Ç–æ—Ä—ñ—Ç—å –ø—ñ–∑–Ω—ñ—à–µ.")
            await state.clear()
            return
            
        await state.update_data(
            topic=topic,
            quiz_answer=lesson_data['quiz_answer'],
            quiz_hint=lesson_data['quiz_hint']
        )
        
        lesson_message = (
            f"üéì **–£–†–û–ö: {lesson_data['lesson_title']}**\n\n"
            f"{lesson_data['content']}\n\n"
            f"---------------------------------------\n"
            f"‚ùì **–í–Ü–ö–¢–û–†–ò–ù–ê:**\n"
            f"*{lesson_data['quiz_question']}* \n\n"
            f"–ù–∞–¥—ñ—à–ª—ñ—Ç—å –í–∞—à—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å, —â–æ–± –ø–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –∑–Ω–∞–Ω–Ω—è."
        )
        await callback_query.message.answer(lesson_message, parse_mode=ParseMode.MARKDOWN)
        await state.set_state(LearningState.in_session)

        
    elif callback_query.data == 'learn_change_topic':
        await state.clear()
        await command_start_learning(callback_query.message, state)
    
    await callback_query.message.edit_reply_markup(reply_markup=None) 

@dp.message(Command("analyze_cutlery"))
async def command_start_cutlery_analysis(message: types.Message, state: FSMContext):
    await state.set_state(CutleryAnalysis.waiting_for_url_or_description)
    await message.answer(
        "üîé **–ê–Ω–∞–ª—ñ–∑ –Ω–∞ –°—Ä—ñ–±–ª–æ (–ü—Ä–æ–±–∞ –ú–µ—Ç–∞–ª—É)**\n\n"
        "–ë—É–¥—å –ª–∞—Å–∫–∞, –Ω–∞–¥—ñ—à–ª—ñ—Ç—å **URL-–∞–¥—Ä–µ—Å—É** –ø—Ä–æ–¥—É–∫—Ç—É –ê–ë–û **—Ç–µ–∫—Å—Ç–æ–≤–∏–π –æ–ø–∏—Å** (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, '–ù–∞–±—ñ—Ä —Å—Ç–æ–ª–æ–≤–∏—Ö –ø—Ä–∏–ª–∞–¥—ñ–≤ —Å—Ä—ñ–±–ª–æ 925 –ø—Ä–æ–±–∏')."
    )

@dp.message(CutleryAnalysis.waiting_for_url_or_description)
async def process_url_or_description(message: types.Message, state: FSMContext, session: aiohttp.ClientSession):
    user_input = message.text.strip()
    await state.clear() 

    if re.match(r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+', user_input):
        await message.answer(f"‚è≥ –ó–∞–ø—É—Å–∫–∞—é —Å–∏–º—É–ª—è—Ü—ñ—é –≤–µ–±-—Å–∫—Ä–µ–π–ø—ñ–Ω–≥—É –¥–ª—è –ø–æ—Å–∏–ª–∞–Ω–Ω—è: `{user_input}`...", parse_mode=ParseMode.MARKDOWN)
        
        content = await fetch_page_content(session, user_input)
        if content:
            found_keywords = analyze_for_silver(content)
            if found_keywords:
                result_message = f"‚úÖ **–ê–ù–ê–õ–Ü–ó –ó–ê–í–ï–†–®–ï–ù–û: –°–†–Ü–ë–õ–û –í–ò–Ø–í–õ–ï–ù–û**\n\n–ó–Ω–∞–π–¥–µ–Ω—ñ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞: *{', '.join(found_keywords)}*."
            else:
                result_message = "‚ùå **–ê–ù–ê–õ–Ü–ó –ó–ê–í–ï–†–®–ï–ù–û: –°–†–Ü–ë–õ–û –ù–ï –í–ò–Ø–í–õ–ï–ù–û** (–ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω—ñ)."
        else:
            result_message = "‚ö†Ô∏è **–ü–û–ú–ò–õ–ö–ê –°–∫—Ä–µ–π–ø—ñ–Ω–≥—É**"
    
    else:
        await message.answer("‚úçÔ∏è –ü—Ä–æ–≤–æ–¥–∂—É —à–≤–∏–¥–∫–∏–π –∞–Ω–∞–ª—ñ–∑ –Ω–∞–¥–∞–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É...")
        found_keywords = analyze_for_silver(user_input)
        if found_keywords:
            result_message = f"‚úÖ **–¢–ï–ö–°–¢–û–í–ò–ô –ê–ù–ê–õ–Ü–ó: –Ü–ú–û–í–Ü–†–ù–û –°–†–Ü–ë–õ–û**\n\n–ó–Ω–∞–π–¥–µ–Ω—ñ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞: *{', '.join(found_keywords)}*."
        else:
            result_message = "‚ùå **–¢–ï–ö–°–¢–û–í–ò–ô –ê–ù–ê–õ–Ü–ó: –°–†–Ü–ë–õ–û –ù–ï –ó–ù–ê–ô–î–ï–ù–û**"

    await message.answer(result_message, parse_mode=ParseMode.MARKDOWN)

@dp.message(Command("base"))
async def command_base_handler(message: types.Message, state: FSMContext):
    await state.clear() 
    await message.answer("**üìö –î–æ–¥–∞–≤–∞–Ω–Ω—è –¥–æ –ë–∞–∑–∏ –ó–Ω–∞–Ω—å (–ö—Ä–æ–∫ 1/2):** –ù–∞–¥—ñ—à–ª—ñ—Ç—å –µ—Ç–∞–ª–æ–Ω–Ω–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ü—ñ–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥–º–µ—Ç–∞. –¶–µ –ø–æ–∫—Ä–∞—â–∏—Ç—å —Ä–æ–±–æ—Ç—É AI Vision —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ –¥–ª—è –í–∞—Å.")
    await state.set_state(BaseForm.waiting_for_photo)

@dp.message(BaseForm.waiting_for_photo, F.photo)
async def handle_base_photo(message: types.Message, state: FSMContext, bot: Bot, session: aiohttp.ClientSession, pool: asyncpg.Pool):
    photo_file_id = message.photo[-1].file_id     
    base64_image = await get_image_base64(session, None, bot, photo_file_id)
    if not base64_image:
        await message.answer("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∞–±–æ —Ñ–∞–π–ª –∑–∞–≤–µ–ª–∏–∫–∏–π.")
        await state.clear()
        return
        
    analysis_schema = {"type": "OBJECT", "properties": {"title": {"type": "STRING"}, "keywords": {"type": "ARRAY", "items": {"type": "STRING"}}, "estimated_value_text": {"type": "STRING"}}}
    
    prompt = "–ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π—Ç–µ —Ü–µ –µ—Ç–∞–ª–æ–Ω–Ω–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è. –ó–≥–µ–Ω–µ—Ä—É–π—Ç–µ –Ω–∞–∑–≤—É, –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ —Ç–∞ –¥—ñ–∞–ø–∞–∑–æ–Ω –≤–∞—Ä—Ç–æ—Å—Ç—ñ."
    ai_base_analysis = await gemini_vision_analysis(session, prompt, base64_image, pool, message.from_user.id, analysis_schema) or {}
    
    ai_keywords = ", ".join(ai_base_analysis.get('keywords', []))
    ai_title = ai_base_analysis.get('title', "–ù/–î")
    ai_value = ai_base_analysis.get('estimated_value_text', "–ù/–î")

    await state.update_data(photo_file_id=photo_file_id, user_id=message.from_user.id, ai_keywords=ai_keywords, ai_title=ai_title, ai_value=ai_value)
    
    base_text = (
        f"**‚úÖ –ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è –æ—Ç—Ä–∏–º–∞–Ω–æ (–ö—Ä–æ–∫ 2/2).**\n\n"
        f"**üî• AI Vision –ó–≥–µ–Ω–µ—Ä—É–≤–∞–≤:**\n"
        f"–ù–∞–∑–≤–∞: `{ai_title}`\n"
        f"–ö–ª—é—á–æ–≤—ñ –°–ª–æ–≤–∞: `{ai_keywords}`\n"
        f"–û—Ü—ñ–Ω–∫–∞: `{ai_value}`\n\n"
        f"–ü—ñ–¥—Ç–≤–µ—Ä–¥—å—Ç–µ –∞–±–æ –≤—ñ–¥—Ä–µ–¥–∞–≥—É–π—Ç–µ —Ü–µ–π –æ–ø–∏—Å (–Ω–∞–¥—ñ—à–ª—ñ—Ç—å –æ—Å—Ç–∞—Ç–æ—á–Ω–∏–π —Ç–µ–∫—Å—Ç)."
    )
    await message.answer(base_text, parse_mode=ParseMode.MARKDOWN)
    await state.set_state(BaseForm.waiting_for_text)

@dp.message(BaseForm.waiting_for_text, F.text)
async def handle_base_text(message: types.Message, state: FSMContext, pool: asyncpg.Pool):
    data = await state.get_data()
    final_text = message.text.strip()
    
    final_title = data.get('ai_title', final_text.split('\n')[0].strip())
    final_keywords = data.get('ai_keywords', final_title)
    final_value_text = data.get('ai_value', final_keywords)
        
    if len(final_text) > 50:
        lines = final_text.split('\n')
        final_title = lines[0].strip()
        final_keywords = final_text 
        final_value_text = final_text 
        
    async with pool.acquire() as conn:
        await conn.execute(
            "INSERT INTO user_base (user_id, title, keywords, estimated_value_text, image_url) VALUES ($1, $2, $3, $4, $5)",
            data['user_id'], final_title, final_keywords, final_value_text, data['photo_file_id']
        )
    await message.answer(f"‚úÖ –ï—Ç–∞–ª–æ–Ω `{final_title}` —É—Å–ø—ñ—à–Ω–æ –¥–æ–¥–∞–Ω–æ –¥–æ –ë–∞–∑–∏ –ó–Ω–∞–Ω—å! –¶–µ –∑—Ä–æ–±–∏—Ç—å –í–∞—à—ñ –º–∞–π–±—É—Ç–Ω—ñ –∞–Ω–∞–ª—ñ–∑–∏ —â–µ —Ç–æ—á–Ω—ñ—à–∏–º–∏.")
    await state.clear()

@dp.callback_query(lambda c: c.data and c.data.startswith('fb_'))
async def process_callback_feedback(callback_query: types.CallbackQuery, pool: asyncpg.Pool):
    try:
        action = callback_query.data.split('_')[1]
        olx_id = callback_query.data.split('_')[2]
        is_like = (action == 'like')
        
        async with pool.acquire() as conn:
            await conn.execute("INSERT INTO user_feedback (user_id, olx_id, is_like) VALUES ($1, $2, $3)",
                               callback_query.from_user.id, olx_id, is_like)
            
            correction_factor = await _get_feedback_correction_factor(pool, olx_id)
            
            await conn.execute("""
                UPDATE olx_posts 
                SET ai_analysis_json = jsonb_set(ai_analysis_json, '{feedback_multiplier}', $1::jsonb)
                WHERE olx_id = $2
            """, json.dumps(correction_factor), olx_id)
            
        await callback_query.answer(f"–î—è–∫—É—î–º–æ –∑–∞ –≤—ñ–¥–≥—É–∫! {'üëç' if is_like else 'üëé'} –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –Ω–∞–≤—á–∞–Ω–Ω—è –æ–Ω–æ–≤–ª–µ–Ω–æ!")
        
        await callback_query.message.edit_reply_markup(reply_markup=None) 
        
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ –∑–≤–æ—Ä–æ—Ç–Ω–æ–≥–æ –∑–≤'—è–∑–∫—É: {e}")
        await callback_query.answer("–ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ –≤—ñ–¥–≥—É–∫—É.")


async def monitoring_worker(bot: Bot, pool: asyncpg.Pool, economic_engine: EconomicEngine, session: aiohttp.ClientSession, interval=600):
    logger.info(f"–í–æ—Ä–∫–µ—Ä –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É –∑–∞–ø—É—â–µ–Ω–æ. –Ü–Ω—Ç–µ—Ä–≤–∞–ª: {interval} —Å–µ–∫.")
    
    while True:
        try:
            spot_prices = EconomicConfig.SPOT_PRICES
            
            new_posts = await fetch_olx_data(session, pool)
                
            async with pool.acquire() as conn:
                    
                for post in new_posts:
                    if not post['olx_id'].startswith('SIM_'):
                         is_exist = await conn.fetchval("SELECT olx_id FROM olx_posts WHERE olx_id = $1", post['olx_id'])
                         if is_exist:
                            continue

                    try:
                        ai_result = await economic_engine.analyze_olx_item(
                            session, post, spot_prices, bot
                        )
                    except Exception as e:
                        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –∞–Ω–∞–ª—ñ–∑—É OLX item {post['olx_id']}: {e}")
                        ai_result = {"is_relevant": False, "type": "–ü–æ–º–∏–ª–∫–∞ –ê–Ω–∞–ª—ñ–∑—É", "deal_assessment": f"–ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}"}

                    is_relevant = ai_result.get('is_relevant', False)
                    
                    await conn.execute(
                        "INSERT INTO olx_posts (olx_id, title, price, published_at, ai_analysis_json, is_relevant) VALUES ($1, $2, $3, $4, $5, $6) ON CONFLICT (olx_id) DO NOTHING",
                        post['olx_id'], post['title'], post['price'], datetime.now(KYIV_TZ), json.dumps(ai_result), is_relevant
                    )
                    
                    if is_relevant and CHANNEL_ID:
                        await send_olx_post(bot, post, ai_result)
                        await asyncio.sleep(5) 

                for feed_name, feed_url in EconomicConfig.RSS_FEEDS.items():
                    content = await fetch_page_content(session, feed_url)
                    
                    if content:
                        try:
                            feed = feedparser.parse(content)
                            for entry in feed.entries[:1]: 
                                title = entry.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫—É')
                                link = entry.get('link', '#')
                                
                                is_silver_related = any(kw in (title).lower() for kw in EconomicConfig.SILVER_KEYWORDS)
                                
                                if is_silver_related and CHANNEL_ID:
                                    post_text = f"üîî **[–ï–õ–Ü–¢–ù–ò–ô –ê–ù–ê–õ–Ü–ó]** –ó–Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤–∏–Ω—É, –ø–æ–≤'—è–∑–∞–Ω—É –∑—ñ —Å—Ä—ñ–±–ª–æ–º/–º–µ—Ç–∞–ª–∞–º–∏:\n\nüì∞ **{title}**\n[–ß–∏—Ç–∞—Ç–∏ –±—ñ–ª—å—à–µ]({link})"
                                    await bot.send_message(
                                        chat_id=CHANNEL_ID, text=post_text, parse_mode=ParseMode.MARKDOWN, disable_web_page_preview=True
                                    )
                                    logger.info(f"–û–ø—É–±–ª—ñ–∫–æ–≤–∞–Ω–æ –Ω–æ–≤–∏–Ω—É –ø—Ä–æ —Å—Ä—ñ–±–ª–æ: {title}")
                                    await asyncio.sleep(2)
                        except Exception as e:
                            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É RSS –¥–ª—è {feed_name}: {e}")
                    else:
                        logger.warning(f"–ù–µ–º–æ–∂–ª–∏–≤–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –≤–º—ñ—Å—Ç RSS-—Å—Ç—Ä—ñ—á–∫–∏: {feed_name}")


        except Exception as e:
            logger.error(f"–ì–ª–æ–±–∞–ª—å–Ω–∞ –ø–æ–º–∏–ª–∫–∞ —É –≤–æ—Ä–∫–µ—Ä—ñ –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É: {e}")
                
        await asyncio.sleep(interval) 


async def main():
    if not BOT_TOKEN or not DATABASE_URL or CHANNEL_ID is None:
        logger.critical("–ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞: –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ BOT_TOKEN, DATABASE_URL –∞–±–æ CHANNEL_ID.")
        sys.exit(1)
        
    try:
        pool = await asyncpg.create_pool(DATABASE_URL)
    except Exception as e:
        logger.critical(f"–ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ –ë–î: {e}")
        sys.exit(1)
        
    await init_db(pool)
    
    bot = Bot(token=BOT_TOKEN, default=DefaultBotProperties(parse_mode=ParseMode.MARKDOWN))
    
    async with aiohttp.ClientSession() as session:
        economic_engine = EconomicEngine(pool)

        dp.message.outer_middleware.register(lambda handler, event, data: {**data, 'session': session, 'pool': pool, 'conn': pool, 'bot': bot, 'economic_engine': economic_engine})
        dp.callback_query.outer_middleware.register(lambda handler, event, data: {**data, 'pool': pool, 'bot': bot, 'session': session})
        
        if CHANNEL_ID and CHANNEL_ID != ADMIN_ID:
             await bot.send_message(
                chat_id=CHANNEL_ID,
                text="ü§ñ **–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ V10 (–ê–¥–∞–ø—Ç–∏–≤–Ω–∞ –∑ –ó–∞–≥–ª—É—à–∫–∞–º–∏) –∑–∞–ø—É—â–µ–Ω–∞!** üõ†Ô∏è‚ú® –§–æ–Ω–æ–≤–∏–π –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ –∞–∫—Ç–∏–≤—ñ–≤ —Ä–æ–∑–ø–æ—á–∞—Ç–æ. **–ó–∞–≥–ª—É—à–∫–∏ –∞–∫—Ç–∏–≤–Ω—ñ** –¥–ª—è –æ–±—Ö–æ–¥—É 403 –ø–æ–º–∏–ª–æ–∫.",
                parse_mode=ParseMode.MARKDOWN
            )
        
        tasks = [
            asyncio.create_task(monitoring_worker(bot, pool, economic_engine, session)),
            dp.start_polling(bot)
        ]

        logger.info("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω–æ. –ü–æ—á–∏–Ω–∞—é –æ–ø–∏—Ç—É–≤–∞–Ω–Ω—è...")
        await asyncio.gather(*tasks)

    await pool.close()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except Exception as e:
        if "terminated by other getUpdates request" in str(e):
             logger.critical("‚ùå **–ö–†–ò–¢–ò–ß–ù–ê –ü–û–ú–ò–õ–ö–ê –ö–û–ù–§–õ–Ü–ö–¢–£:** –í–∏—è–≤–ª–µ–Ω–æ 'TelegramConflictError'. –¶–µ –æ–∑–Ω–∞—á–∞—î, —â–æ **–∑–∞–ø—É—â–µ–Ω–æ –¥–≤–∞ –∞–±–æ –±—ñ–ª—å—à–µ –µ–∫–∑–µ–º–ø–ª—è—Ä–∏ –±–æ—Ç–∞ –æ–¥–Ω–æ—á–∞—Å–Ω–æ**. –ë—É–¥—å –ª–∞—Å–∫–∞, –ø–µ—Ä–µ–∫–æ–Ω–∞–π—Ç–µ—Å—è, —â–æ –Ω–∞ –í–∞—à–æ–º—É —Ö–æ—Å—Ç–∏–Ω–≥—É –ø—Ä–∞—Ü—é—î –ª–∏—à–µ –æ–¥–∏–Ω –µ–∫–∑–µ–º–ø–ª—è—Ä.")
        else:
             logger.critical(f"–ì–æ–ª–æ–≤–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è: {e}")